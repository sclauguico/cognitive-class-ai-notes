{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 2: Preparing Data\n",
    "\n",
    "## Statistics, Random data and Sampling on Data Frames\n",
    "\n",
    "### Lesson Objectives \n",
    "\n",
    "After completing this lesson, you should be able to: \n",
    "\n",
    "- Compute column summary statistics\n",
    "-\tCompute pairwise statistics between series/columns\n",
    "-\tPerform standard sampling on any `DataFrame` \n",
    "-\tSplit any `DataFrame` randomly into subsets\n",
    "-\tPerform stratified sampling onto `DataFrames`\n",
    "-\tGenerate Random Data from Uniform and Normal Distributions \n",
    "\n",
    "\n",
    "## Summary Statistics for DataFrames \n",
    "\n",
    "-\tColumn summary statistics for DataFrames are available through DataFrame's `describe()` method\n",
    "-\tIt returns another `DataFrame`, which contains column-wise results for: \n",
    "-\t`min`, `max`\n",
    "-\t`mean`, `stddev`\n",
    "-\t`count`\n",
    "- Column summary statistics can also be computed through DataFrame's `groupBy()` and `agg()` methods, but stddev is not supported\n",
    "-\tIt also returns another `DataFrame` with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4041\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1669602192561)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@263fd6db\r\n",
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/28 10:23:23 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\n",
      "22/11/28 10:23:30 ERROR Executor: Exception in task 1.0 in stage 1.0 (TID 2)\n",
      "java.lang.ClassCastException: class $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record cannot be cast to class $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record ($line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @88d0cc1; $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @47cb3825)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "22/11/28 10:23:30 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2) (host.docker.internal executor driver): java.lang.ClassCastException: class $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record cannot be cast to class $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record ($line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @88d0cc1; $line10.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Record is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @47cb3825)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "22/11/28 10:23:30 ERROR TaskSetManager: Task 1 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2) (host.docker.internal executor driver): java.lang.ClassCastException: class Record cannot be cast to class Record (Record is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @88d0cc1; Record is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @47cb3825)\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2) (host.docker.internal executor driver): java.lang.ClassCastException: class Record cannot be cast to class Record (Record is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @88d0cc1; Record is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @47cb3825)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:808)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:767)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:776)\r",
      "  ... 40 elided\r",
      "Caused by: java.lang.ClassCastException: class Record cannot be cast to class Record (Record is in unnamed module of loader org.apache.spark.repl.ExecutorClassLoader @88d0cc1; Record is in unnamed module of loader scala.tools.nsc.interpreter.IMain$TranslatingClassLoader @47cb3825)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:136)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "case class Record(desc: String, value1: Int, value2: Double)\n",
    "\n",
    "val records = Array(Record(\"first\",1,3.7), Record(\"second\",-2,2.1), Record(\"third\",6,0.7))\n",
    "\n",
    "val recRDD = sc.parallelize(records)\n",
    "val recDF = spark.createDataFrame(recRDD)\n",
    "recDF.show()\n",
    "\n",
    "val recStats = recDF.describe()\n",
    "recStats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "// Fetching Results from DataFrame\n",
    "\n",
    "recStats.filter($\"summary\" === \"stddev\").first()\n",
    "  \n",
    "val ar1 = recStats.filter($\"summary\" === \"stddev\").first().toSeq.drop(2).map(_.toString.toDouble).toArray\n",
    "println(ar1)\n",
    " \n",
    "val ar2 = recStats.select(\"value1\").map(s => s(0).toString.toDouble).collect()\n",
    "println(ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "recDF.groupBy().agg(Map(\"value1\" -> \"min\", \"value1\" -> \"max\" ))\n",
    " \n",
    "recDF.groupBy().agg(Map(\"value1\" -> \"min\", \"value2\" -> \"min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val recStatsGroup = recDF.groupBy().agg(min(\"value1\"), min(\"value2\"))\n",
    "\n",
    "recStatsGroup.columns\n",
    " \n",
    "recStatsGroup.first().toSeq.toArray.map(_.toString.toDouble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Statistics on DataFrames \n",
    "\n",
    "-\tMore statistics are available through the stats method in a DataFrame \n",
    "-\tIt returns a `DataFrameStatsFunctions` object, which has the following methods:\n",
    "-\t`corr()` - computes Pearson correlation between two columns\n",
    "-\t`cov()` - computes sample covariance between two columns\n",
    "- `crosstab()` - Computes a pair-wise frequency table of the given columns \n",
    "- `freqItems()` - finds frequent items for columns, possibly with false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val  recDFStat = recDF.stat\n",
    "\n",
    "println(\"correlation value1 and value2\",recDFStat.corr(\"value1\", \"value2\"))\n",
    "println(\"correlation value1 and value2\",recDFStat.cov(\"value1\", \"value2\"))\n",
    "recDFStat.freqItems(Seq(\"value1\"), 0.3) .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling on DataFrames \n",
    "\n",
    "-\tCan be performed on any `DataFrame`\n",
    "-\tReturns a sampled subset of a `DataFrame`\n",
    "-\tSampling with or without replacement\n",
    "- Fraction: expected fraction of rows to generate\n",
    "-\tCan be used on bootstrapping procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "val df = spark.createDataFrame(Seq((1, 10), (1, 20), (2, 10), (2, 20), (2, 30), (3, 20), (3, 30))).toDF(\"key\", \"value\")\n",
    "df.show()\n",
    "val dfSampled = df.sample(withReplacement=false, fraction=0.3, seed=11L)\n",
    "dfSampled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Split on DataFrames\n",
    "\n",
    "-\tCan be performed on any DataFrame\n",
    "-\tReturns an array of DataFrames\n",
    "-\tWeighs for the split will be normalized if the do not add up to 1\n",
    "-\tUseful for splitting a data set into training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "val dfSplit = df.randomSplit(weights=Array(0.3, 0.7), seed=11L) \n",
    "\n",
    "dfSplit(0).show()\n",
    "dfSplit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling on DataFrames \n",
    "\n",
    "-\tCan be performed on any `DataFrame` \n",
    "- Any column may work as key\n",
    "-\tWithout replacement\n",
    "-\tFraction: specified by key\n",
    "-\tAvailable as `sampleBy` function in `DataFrameStatFunctions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "df.stat.\n",
    "    sampleBy(col=\"key\",\n",
    "    fractions=Map(1 -> 0.7, 2 -> 0.7, 3 -> 0.7),\n",
    "    seed=11L).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Data Generation \n",
    "\n",
    "-\tSQL functions to generate columns filled with random values \n",
    "-\tTwo supported distributions: uniform and normal\n",
    "-\tUseful for randomized algorithms, prototyping and performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import  org.apache.spark.sql.functions.{rand, randn}\n",
    "\n",
    "val df = spark.range(0, 10)\n",
    "\n",
    "df.select(\"id\").withColumn(\"uniform\", rand(10L)).\n",
    "    withColumn(\"normal\", randn(10L)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "-\tHaving completed this lesson, you should be able to:\n",
    "- Compute column summary statistics \n",
    "-\tCompute pairwise statistics between series/columns\n",
    "-\tPerform standard sampling on any `DataFrame` \n",
    "-\tSplit any `DataFrame` randomly into subsets \n",
    "-\tPerform stratified sampling on `DataFrames`\n",
    "-\tGenerate Random Data from Uniform and Normal Distributions\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
