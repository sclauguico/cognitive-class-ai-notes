{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Basic Statistics and Data Types - Sampling \n",
    " \n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "-\tAfter completing this lesson, you should be able to:\n",
    "-\tPerform standard sampling on any RDD \n",
    "-\tSplit any RDD randomly into subsets \n",
    "-\tPerform stratified sampling on RDDs of key-value pairs \n",
    "\n",
    "\n",
    "## Sampling \n",
    "\n",
    "-\tCan be performed on any RDD \n",
    "- Returns a sampled subset of an RDD\n",
    "-\tSampling with or without replacement\n",
    "- Fraction:\n",
    "-\twithout replacement - expected size of the sample as fraction of RDD's size \n",
    "-\twith replacement - expected number of times each element is chosen\n",
    "-\tCan be used on bootstrapping procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://host.docker.internal:4043\n",
       "SparkContext available as 'sc' (version = 3.3.0, master = local[*], app id = local-1669534598054)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\r\n",
       "import org.apache.spark.rdd.RDD\r\n",
       "elements: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[0] at parallelize at <console>:29\r\n",
       "res0: Array[org.apache.spark.mllib.linalg.Vector] = Array([3.0,-11.0,19.0])\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A Simple Sampling \n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val elements: RDD[Vector] = sc.parallelize(Array(\n",
    "    Vectors.dense(4.0,7.0,13.0),\n",
    "    Vectors.dense(-2.0,8.0,4.0),\n",
    "    Vectors.dense(3.0,-11.0,19.0)))\n",
    "\n",
    "elements.sample(withReplacement=false, fraction=0.5, seed=10L).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[org.apache.spark.mllib.linalg.Vector] = Array([4.0,7.0,13.0], [-2.0,8.0,4.0], [3.0,-11.0,19.0])\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.sample(withReplacement=false, fraction=0.5, seed=7L).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[org.apache.spark.mllib.linalg.Vector] = Array([4.0,7.0,13.0], [3.0,-11.0,19.0])\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.sample(withReplacement=false, fraction=0.5, seed=64L).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Split\n",
    "\n",
    "-\tCan be performed on any RDD\n",
    "-\tReturns an array of RDDs\n",
    "- Weights for the split will be normalized if they do not add up to 1\n",
    "-\tUseful for splitting a data set into training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/27 15:36:48 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:26\r\n",
       "splits: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[5] at randomSplit at <console>:27, MapPartitionsRDD[6] at randomSplit at <console>:27, MapPartitionsRDD[7] at randomSplit at <console>:27)\r\n",
       "training: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at randomSplit at <console>:27\r\n",
       "test: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[6] at randomSplit at <console>:27\r\n",
       "validation: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at randomSplit at <console>:27\r\n",
       "res3: Array[Long] = Array(600491, 199608, 199901)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.parallelize(1 to 1000000)\n",
    "val splits = data.randomSplit(Array(0.6, 0.2, 0.2), seed = 13L)\n",
    "\n",
    "val training = splits(0)\n",
    "val test = splits(1)\n",
    "val validation = splits(2)\n",
    "\n",
    "splits.map(_.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling \n",
    "\n",
    "-\tCan be performed on RDDs of key-value pairs \n",
    "-\tThink of keys as labels and values as an specific attribute\n",
    "- Two supported methods defined in `PairRDDFunctions`:\n",
    "-\t`sampleByKey` requires only one pass over the data and provides an expected sample size\n",
    "-\t`sampleByKeyExact` provides the exact sampling size with 99.99% confidence but requires significantly more resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.mllib.linalg.distributed.IndexedRow\r\n",
       "rows: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.distributed.IndexedRow] = ParallelCollectionRDD[8] at parallelize at <console>:29\r\n",
       "fractions: Map[Long,Double] = Map(0 -> 1.0, 1 -> 0.5)\r\n",
       "approxSample: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[10] at sampleByKey at <console>:38\r\n",
       "res4: Array[(Long, org.apache.spark.mllib.linalg.Vector)] = Array((0,[1.0,2.0]), (1,[4.0,5.0]), (1,[7.0,8.0]))\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.linalg.distributed.IndexedRow\n",
    "\n",
    "val rows: RDD[IndexedRow] = sc.parallelize(Array(\n",
    "    IndexedRow(0, Vectors.dense(1.0,2.0)),\n",
    "    IndexedRow(1, Vectors.dense(4.0,5.0)),\n",
    "    IndexedRow(1, Vectors.dense(7.0,8.0))))\n",
    "\n",
    "val fractions: Map[Long, Double] = Map(0L -> 1.0, 1L -> 0.5)\n",
    "\n",
    "val approxSample = rows.map{\n",
    "    case IndexedRow(index, vec) => (index, vec)\n",
    "}.sampleByKey(withReplacement = false, fractions, 9L)\n",
    "\n",
    "approxSample.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary: \n",
    "\n",
    "-\tHaving completed this lesson, you should now be able to:\n",
    "-\tPerform standard sampling on any RDD\n",
    "-\tSplit any RDD randomly into subsets\n",
    "-\tPerform stratified sampling on RDDs of key-value pairs\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
